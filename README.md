# chestnut-detection

This is an implementation of the method proposed in
> Keyhan Najafian, Alireza Ghanbari, Ian Stavness, Lingling Jin, Gholam Hassan Shirdel, and Farhad Maleki. Semi-self-supervised Learning Approach for Wheat Head Detection using Extremely Small Number of Labeled Samples. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 1342-1351, 2021.

First, you have to conduct pixel-wise annotation, just like you would do in semantic segmentation (but for  only a few frames in the video).
Here we assume that you use [labelme](https://github.com/wkentaro/labelme).

After completing annotation, run the following command to export the labels in the VOC format.
```
python labelme2voc.py [input_dir] dataset_voc --labels labels.txt
```
Here, `input_dir` is where you've put the `.json` files generated by labelme.

Now you've got the labels in `dataset_voc/SegmentationClass`. So what you do next is:
```
python voc2object_wise_mask.py dataset_voc/SegmentationClass masks labels.txt
```
This transforms VOC-format labels into object-wise masks for each class, e.g.
```
masks/[class1].npy: (n_object of class1, height, width)
masks/[class2].npy: (n_object of class2, height, width)
...
```
Now you can use `torchvision.ops.masks_to_boxes` to convert the masks into bounding boxes used in object detection problems.