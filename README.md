# chestnut-detection

This is an implementation of the method proposed in
> Keyhan Najafian, Alireza Ghanbari, Ian Stavness, Lingling Jin, Gholam Hassan Shirdel, and Farhad Maleki. Semi-self-supervised Learning Approach for Wheat Head Detection using Extremely Small Number of Labeled Samples. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 1342-1351, 2021.

1. Prepare your videos. Put video clips of backgrounds in `back` directory, fields in 'field'.

2. Choose 1 or more "representative frames" from each video clip of fields. Put them in `rep` directory.

3. Make pixel-wise labels for each representative image, just like you would do in semantic segmentation.

4. Export the labels in Pascal VOC format and put them in `mask` directory. If you have made the labels with [labelme](https://github.com/wkentaro/labelme), you can export them with the following command:
```
python labelme2voc.py [input_dir] dataset_voc --labels labels.txt
```
where`input_dir` is where you've put the `.json` files generated by labelme.
Now you've got `.npy` files in `dataset_voc/SegmentationClass`.
Note that `labelme2voc.py` is taken from [the original labelme repo](https://github.com/wkentaro/labelme/blob/main/examples/semantic_segmentation/labelme2voc.py) and fixed a little bit to handle wider range of images.

5. Create a text file named `labels.txt` and list class names in it. For example, it will look like this if you are working on chestnut detection:
```
nut(fine)
nut(empty)
burr
burr+nut
``

6. make sure your project directory is organized as below:

```
project_root
│   
└───back
│   │   background_video1.mp4
│   │   background_video2.mp4
│   │   ...
│   
└───field
│   │   field_video1.mp4
│   │   field_video2.mp4
│   │	...
│
└───rep # representative frames taken from each field video
│   │   field_video1_0031.png # [video_name]_[timestamp].png
│   │   field_video1_0118.png
│   │	field_video2_0005.png
│   │	...
│
└───mask
│   │   field_video1_0031.npy
│   │   field_video1_1018.npy
│   │	field_video2_0005.npy
│   │	...
│
└───labels.txt
```

5. Generate a composite dataset & a dataset for the 1st step of domain adaptation with the following command.
```
python make_dataset.py -n [n_sample] -p [prob_1 prob_2 ... prob_n_class] --root [path to the root directory of your project] -o composite --verbose --bbox --cuda  --domain-adaptation domain_adaptation_1
```
